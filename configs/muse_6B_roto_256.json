{
  "layers": 28,
  "d_model": 4096,
  "n_heads": 16,
  "n_vocab": 50400,
  "norm": "layernorm",
  "pe": "rotary",
  "pe_rotary_dims": 64,
  "seq": 2048,
  "cores_per_replica": 8,
  "per_replica_batch": 1,
  "gradient_accumulation_steps": 32,
  "warmup_steps": 3000,
  "anneal_steps": 2795,
  "lr": 5e-5,
  "end_lr": 1e-5,
  "weight_decay": 0.1,
  "total_steps": 5795,
  "tpu_size": 8,
  "bucket": "muse-tpu-storage",
  "model_dir": "muse-checkpoints",
  "train_set": "muse.train.index",
  "val_set": {
    "pile": "muse.val.index"
  },
  "eval_harness_tasks": [],
  "val_batches": 1836,
  "val_every": 50,
  "ckpt_every": 50,
  "keep_every": 1000,
  "name": "muse_GPT3_6B_pile_rotary",
  "wandb_project": "MUSE",
  "comment": ""
}
